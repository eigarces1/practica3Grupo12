
# üìä Clase: Tratamiento de Datos  
**Grupo #12**


### üë• Integrantes  
- Martha Aguilar  
- Elian Garces  

---

Ôªø# **Practica #3**
 
# üõ°Ô∏è Informe sobre el Dataset *Global Cybersecurity Threats (2015‚Äì2024)*  

## üìå Prop√≥sito del Dataset  
El presente informe se basa en el dataset **[Global Cybersecurity Threats (2015‚Äì2024)](https://www.kaggle.com/datasets/atharvasoundankar/global-cybersecurity-threats-2015-2024)** disponible en *Kaggle*.  

Nuestro grupo eligi√≥ este conjunto de datos porque constituye una excelente fuente para el an√°lisis de ciberseguridad, ya que:  

- Dispone de una base robusta con m√∫ltiples variables relevantes.  
- Refleja c√≥mo los ataques cibern√©ticos han impactado a diferentes industrias a nivel global.  
- Permite realizar tanto an√°lisis estad√≠sticos como explicaciones visuales.  
- Se presta para procesos de **limpieza de datos, an√°lisis exploratorio (EDA), visualizaci√≥n y modelado predictivo**.  

En resumen, este dataset resulta ideal para combinar t√©cnicas de tratamiento de datos con aplicaciones pr√°cticas en seguridad inform√°tica.  

---

## üìÇ Descripci√≥n del Dataset  
Este conjunto de datos ofrece una visi√≥n integral de las **amenazas cibern√©ticas globales** entre 2015 y 2024. Contiene informaci√≥n detallada sobre:  

- üî∏ **Tipos de ataques** (phishing, ransomware, DDoS, etc.)  
- üî∏ **Industrias objetivo** (salud, educaci√≥n, finanzas, tecnolog√≠a, etc.)  
- üî∏ **Pa√≠ses afectados**  
- üî∏ **P√©rdidas financieras estimadas**  
- üî∏ **N√∫mero de usuarios afectados**  
- üî∏ **Tiempo de resoluci√≥n de incidentes**  
- üî∏ **Mecanismos de defensa utilizados** (firewalls, IA, VPN, etc.)  
- üî∏ **Vulnerabilidades explotadas** (contrase√±as d√©biles, ingenier√≠a social, zero-day, etc.)  
- üî∏ **Fuentes de ataque** (grupos de hackers, insiders, estados-naci√≥n, etc.)  



# üßπ Limpieza y Transformaci√≥n de Datos

El dataset original **Global Cybersecurity Threats (2015‚Äì2024)** conten√≠a inconsistencias y datos incompletos que requer√≠an preparaci√≥n antes del an√°lisis. El proceso aplicado en el notebook sigui√≥ estas etapas:

---

### 1) Carga e inspecci√≥n inicial
**Objetivo:** Asegurar la disponibilidad del archivo y una primera inspecci√≥n de su estructura.
- Descarga del dataset desde Kaggle mediante la API.
- Lectura del archivo `.csv` en un DataFrame de `pandas`.
- Inspecci√≥n de la estructura (`df.info()`, `df.head()`) y dimensiones (`df.shape`).

---

### 2) Identificaci√≥n de problemas en los datos

**Objetivo:** Entender calidad, completitud y dispersi√≥n de categor√≠as antes de limpiar.

- **Duplicados**: Detecci√≥n con `df.duplicated().sum()`.  
- **Valores nulos**: Revisi√≥n con `df.isnull().sum()`.  
- **Tipos de datos incorrectos**: Algunas columnas num√©ricas estaban como `object`.  
- **Categor√≠as inconsistentes**: Valores con may√∫sculas/min√∫sculas o espacios adicionales.  

---

### 3) Eliminaci√≥n de duplicados

**Objetivo:** Evitar el confunsiones que generan registros repetidos en m√©tricas, agregaciones y modelos.
Se eliminaron filas repetidas para evitar sesgo en el an√°lisis.

```python
df = df.drop_duplicates()

---
### 4) Manejo de valores faltantes

- **Num√©ricos**: Se reemplazaron los `NaN` por `0` en las siguientes columnas:
    - `Financial Loss (in Million $)`
    - `Number of Affected Users`
    - `Incident Resolution Time (in Hours)`
- **Categ√≥ricos**: Se rellenaron los valores nulos con `'Unknown'` para mantener la consistencia.

**Ejemplo aplicado en el notebook:**
```python
df['Financial Loss (in Million $)'].fillna(0, inplace=True)
df['Number of Affected Users'].fillna(0, inplace=True)
df['Incident Resolution Time (in Hours)'].fillna(0, inplace=True)
df[categorical_cols] = df[categorical_cols].fillna('Unknown')

---
### 5) Normalizaci√≥n de texto en categor√≠as

Se limpiaron los strings para evitar duplicidad artificial y estandarizar las entradas.

```python
df[col] = df[col].str.strip().str.title()

Esto permitio que entradas como "phishing", "Phishing " y "PHISHING" queden como un √∫nico valor "Phishing".

---
### 6) Asegurar tipos de datos correctos

**Objetivo:** Un tipado consistente es crucial para realizar c√°lculos precisos, ordenamientos correctos y operaciones de series de tiempo de manera eficiente.
Se realizaron conversiones de tipo para garantizar la consistencia en los datos:
- `Year` ‚Üí `int`
- `Financial Loss (in Million $)` ‚Üí `float`
- `Number of Affected Users` ‚Üí `int`
- `Incident Resolution Time (in Hours)` ‚Üí `int`

---
### 7) Variables derivadas (Feature Engineering)

**Objetivo:** Estas variables permiten un an√°lisis detallado del costo o impacto unitario y una segmentaci√≥n r√°pida de los incidentes seg√∫n su criticidad.
Se crearon nuevas variables para enriquecer el conjunto de datos y permitir an√°lisis m√°s profundos.

#### Impacto econ√≥mico por usuario afectado
- **F√≥rmula**: `Impact Per User ($) = (Financial Loss (in Million $) * 1,000,000) / Number of Affected Users`
- **Condici√≥n**: Se a√±ade una condici√≥n para evitar la divisi√≥n por cero: si `Number of Affected Users` es `0`, el resultado es `0`.

#### Marcador de criticidad
- **Regla**: `IsCritical = 1` si `Incident Resolution Time (in Hours) > 100`, de lo contrario, `0`.
- **Nota**: Este es un umbral ilustrativo que permite segmentar incidentes con alta complejidad o un tiempo de respuesta prolongado.

---
### 8) Listo para an√°lisis y visualizaci√≥n
Con el conjunto de datos depurado y enriquecido, se puede proceder al an√°lisis. Algunos ejemplos de los an√°lisis realizados incluyen:

- **Conteo y tendencias** (por ejemplo, ataques por a√±o).
- **An√°lisis de distribuciones** por tipo de ataque.
- **Creaci√≥n de visualizaciones geogr√°ficas**, como un **choropleth** con Plotly, para mostrar las p√©rdidas econ√≥micas por pa√≠s y su evoluci√≥n temporal.

